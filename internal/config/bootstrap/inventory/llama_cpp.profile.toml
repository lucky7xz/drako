# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ The Llama C++ Forge - A Local Inference Engine                             â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ This profile provides a command center for building and running llama.cpp, â”‚
# â”‚ a powerful inference engine for GGUF models. It includes commands for      â”‚
# â”‚ building from source, running local servers, managing Docker containers,   â”‚
# â”‚ and accessing documentation.                                               â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# â”Œâ”€ Grid Dimensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
X = 3
Y = 3
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜




#=======================================================================================================================
# â”Œâ”€ COLUMN A: Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Compile llama.cpp from source and manage the build directory.
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[[commands]]
name = "Ì¶ðŸ“¦ Ì¶BÌ¶uÌ¶iÌ¶lÌ¶dÌ¶ Ì¶fÌ¶rÌ¶oÌ¶mÌ¶ Ì¶SÌ¶oÌ¶uÌ¶rÌ¶cÌ¶eÌ¶"
description = "Compile llama.cpp using modern CMake. Creates a 'llama.cpp' directory in your home folder."
col = "A"
row = 0
items = [
    { name = "Build (Basic)", description = "Clones the repo and builds the standard version.", command = """
#!/bin/bash
set -e
if [ ! -d "$HOME/llama.cpp" ]; then
  git clone https://github.com/ggerganov/llama.cpp.git "$HOME/llama.cpp"
fi
cd "$HOME/llama.cpp"
cmake -B build
cmake --build build -j
echo "Build complete! Binaries are in $HOME/llama.cpp/build/bin"
""", auto_close_execution = false },
    { name = "Build (CUDA)", description = "Builds with NVIDIA CUDA support for GPU acceleration.", command = """
#!/bin/bash
set -e
if [ ! -d "$HOME/llama.cpp" ]; then
  git clone https://github.com/ggerganov/llama.cpp.git "$HOME/llama.cpp"
fi
cd "$HOME/llama.cpp"
cmake -B build -DLLAMA_CUDA=ON
cmake --build build -j
echo "Build complete! Binaries are in $HOME/llama.cpp/build/bin"
""", auto_close_execution = false },
    { name = "Clean Build", description = "Removes the build directory to start fresh.", command = "rm -rf $HOME/llama.cpp/build", auto_close_execution = true }
]

[[commands]]
name = "ðŸ“¥ Manual Download & Unpack"
description = "Access the official GitHub releases page to download pre-compiled binaries."
col = "A"
row = 1
items = [
  { name = "Open Releases Page", description = "Opens the latest llama.cpp releases page in your browser. Download a pre-compiled release, unpack it, and rename the main directory to 'llama.cpp', placing it in your home folder (~/llama.cpp) to match the expected paths in Quick Start.", command = "xdg-open https://github.com/ggerganov/llama.cpp/releases/latest >/dev/null 2>&1 &", auto_close_execution = true }
]

[[commands]]
name = "ðŸ›‘ Stop All Services"
description = "Stops all running llama-server processes."
col = "A"
row = 2
items = [
  { name = "Stop All Servers", description = "Finds and terminates all running llama-server instances.", command = """
#!/bin/bash
if pgrep -f "llama-server" > /dev/null; then
    pkill -f "llama-server"
    echo "All llama-server processes have been stopped."
else
    echo "No llama-server processes were found running."
fi
""", auto_close_execution = false }
]

#=======================================================================================================================
# â”Œâ”€ COLUMN B: Local Usage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Run inference and benchmarks directly on your local machine.
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[[commands]]
name = "ðŸ“¥ Install Models"
description = "Launch a server to download models. These commands run in the foreground so you can see download progress and accept licenses. Once the model download is complete (server starts), you can safely Ctrl+C. Models are saved to ~/.cache/huggingface/hub/ or ~/.cache/llama.cpp."
col = "B"
row = 0
items = [
    { name = "Download & Run CPU (Qwen3-0.6B)", description = "Downloads the small and fast Qwen3-0.6B model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen3-0.6B-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033", auto_close_execution = false },
    #{ name = "Download & Run CPU (Qwen2.5-VL-3B)", description = "Downloads the Qwen2.5-VL-3B Vision Language model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen2.5-VL-3B-Instruct-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033", auto_close_execution = false },
    { name = "Download & Run CPU (Qwen2.5-Omni-3B-GGUF)", description = "Downloads the Qwen2.5-Omni-3B-GGUF model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen2.5-Omni-3B-GGUF --jinja -c 16000 --host 127.0.0.1 --port 8033", auto_close_execution = false },
    { name = "Download & Run CPU (Qwen3-4B-Thinking)", description = "Downloads the Qwen3-4B-Thinking-2507-Q8_0 model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen3-4B-Thinking-2507-Q8_0-GGUF --jinja -c 16000 --host 127.0.0.1 --port 8033", auto_close_execution = false },
   # { name = "Download & Run CPU (Ministral 3B)", description = "Downloads the Ministral 3B Reasoning model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-3B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033", auto_close_execution = false },
   # { name = "Download & Run CPU (Ministral 8B)", description = "Downloads the Ministral 8B Reasoning model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-8B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033", auto_close_execution = false },
    { name = "Download & Run GPU (GPT-OSS-20B)", description = "Downloads GPT-OSS-20B (MXFP4). Optimized for NVIDIA 3060 (12GB). Runs in foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/gpt-oss-20b-GGUF -m gpt-oss-20b-MXFP4.gguf --jinja -c 8192 -ngl 10 --n-cpu-moe 12 -b 2048 -ub 2048 --cont-batching -fa --no-mmap --host 0.0.0.0 --port 8033", auto_close_execution = false }
]

[[commands]]
name = "ðŸ¦™ Quick Start"
description = "Launch the server with sensible defaults. Assumes models are already downloaded. These run in the background."
col = "B"
row = 1
items = [
    {name = "Open Local WebUI", description = "Opens the LLamaCPP WebUI address in your Browser.", command = "xdg-open http://127.0.0.1:8033/ > /dev/null 2>&1 &"},
    { name = "Start CPU (Qwen3-0.6B)", description = "Launches the small and fast Qwen3-0.6B model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen3-0.6B-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    #{ name = "Start CPU (Qwen2.5-VL-3B)", description = "Launches the Qwen2.5-VL-3B Vision Language model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen2.5-VL-3B-Instruct-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start CPU (Qwen2.5-Omni-3B-GGUF)", description = "Launches the Qwen2.5-Omni-3B-GGUF model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen2.5-Omni-3B-GGUF --jinja -c 16000 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start CPU (Qwen3-4B-Thinking)", description = "Launches the Qwen3-4B-Thinking-2507-Q8_0 model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen3-4B-Thinking-2507-Q8_0-GGUF --jinja -c 16000 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start CPU (Ministral 3B)", description = "Launches the Ministral 3B Reasoning model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-3B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start CPU (Ministral 8B)", description = "Launches the Ministral 8B Reasoning model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-8B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start GPU Server (GPT-OSS-20B)", description = "Launches GPT-OSS-20B. Optimized for NVIDIA 3060 (12GB). Network available (0.0.0.0).", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/gpt-oss-20b-GGUF -m gpt-oss-20b-MXFP4.gguf --jinja -c 8192 -ngl 10 --n-cpu-moe 12 -b 2048 -ub 2048 --cont-batching -fa --no-mmap --host 0.0.0.0 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
   #{ name = "Open Remote WebUI", description = "Opens a (remote) LLamaCPP WebUI address in your Browser.", command = "xdg-open http://0.0.0.0:8033/ > /dev/null 2>&1 &"},
]

[[commands]]
name = "Ì¶ðŸ› ï¸ Ì¶CÌ¶uÌ¶sÌ¶tÌ¶oÌ¶mÌ¶ Ì¶&Ì¶ Ì¶CÌ¶LÌ¶IÌ¶"
description = "Run a custom server or interact directly with a model via the command line."
col = "B"
row = 2
items = [
    { name = "Custom CPU Server", description = "Prompts for model, GPU layers, and context size.", command = """
#!/bin/bash
read -p 'Enter HF Model Repo ID (e.g., ggml-org/Qwen2-7B-Instruct-GGUF): ' model_repo
read -p 'Enter number of GPU layers (e.g., 35): ' ngl
read -p 'Enter context size (e.g., 4096): ' ctx_size
$HOME/llama.cpp/build/bin/llama-server -hf "$model_repo" -c "$ctx_size" --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &
""", auto_close_execution = false },
    { name = "Custom GPU Server", description = "Prompts for model, GPU layers, and context size.", command = """
#!/bin/bash
read -p 'Enter HF Model Repo ID (e.g., ggml-org/Qwen2-7B-Instruct-GGUF): ' model_repo
read -p 'Enter number of GPU layers (e.g., 35): ' ngl
read -p 'Enter context size (e.g., 4096): ' ctx_size
$HOME/llama.cpp/build/bin/llama-server -hf "$model_repo" -c "$ctx_size" -ngl "$ngl" --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &
""", auto_close_execution = false }
]

#=======================================================================================================================
# â”Œâ”€ COLUMN C: Docker & Docs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Manage containerized servers and access documentation.
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[[commands]]
name = "ðŸ’¾ Manage Models"
description = "Manage GGUF models downloaded to the local cache (~/.cache/llama.cpp)."
col = "C"
row = 0
items = [
    { name = "List Models", description = "List all GGUF models and their sizes.", command = """
#!/bin/bash
echo "ðŸ“‚ Location: ~/.cache/llama.cpp/"
echo "---------------------------------------------------"
du -h ~/.cache/llama.cpp/*.gguf | awk '{print $1, $2}' | sed "s|$HOME/.cache/llama.cpp/||"
echo "---------------------------------------------------"
du -sh ~/.cache/llama.cpp
""", auto_close_execution = false },
    { name = "Remove Model", description = "Select and delete a specific model file and its metadata.", command = """
#!/bin/bash
cd ~/.cache/llama.cpp || { echo "Cache dir not found"; exit 1; }
files=(*.gguf)
if [ ${#files[@]} -eq 0 ]; then
    echo "No models found."
    exit 0
fi
echo "Select a model to DELETE:"
select file in "${files[@]}"; do
    if [ -n "$file" ]; then
        read -p "âš ï¸  Are you sure you want to delete '$file'? (y/N) " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            rm -v "$file"
            # Try to remove associated metadata if it exists
            rm -vf "${file}.etag"
        else
            echo "Cancelled."
        fi
        break
    else
        echo "Invalid selection."
    fi
done
""", auto_close_execution = false }
]

[[commands]]
name = "ðŸ“š Documentation"
description = "Access official llama.cpp documentation and resources."
col = "C"
row = 1
items = [
    { name = "Open llama.cpp GitHub", description = "Explore the source code and latest updates.", command = "xdg-open https://github.com/ggerganov/llama.cpp >/dev/null 2>&1 &" },
    { name = "Learn about GGUF", description = "Read about the GGUF model format.", command = "xdg-open https://github.com/ggerganov/ggml/blob/master/docs/gguf.md >/dev/null 2>&1 &"}
]

[[commands]]
name = " â±ï¸ Benchmarks"
description = "Measure model performance with llama-bench."
col = "C"
row = 2
items = [
    { name = "Run Benchmark", description = "Prompts for a model path and parameters to run a benchmark.", command = """
#!/bin/bash
read -p 'Enter path to GGUF model file: ' model_path
read -p 'Enter number of threads (e.g., 8): ' threads
read -p 'Enter number of GPU layers (e.g., 35): ' ngl
$HOME/llama.cpp/build/bin/llama-bench -hf "$model_path" -t "$threads" -ngl "$ngl" > ~/llama.cpp/server.log 2>&1 &
""", auto_close_execution = false }
]
