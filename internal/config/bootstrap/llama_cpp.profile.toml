# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ The Llama C++ Forge - A Local Inference Engine                             â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ This profile provides a command center for building and running llama.cpp, â”‚
# â”‚ a powerful inference engine for GGUF models. It includes commands for      â”‚
# â”‚ building from source, running local servers, managing Docker containers,   â”‚
# â”‚ and accessing documentation.                                               â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# â”Œâ”€ Grid Dimensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
X = 3
Y = 3
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜




#=======================================================================================================================
# â”Œâ”€ COLUMN A: Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Compile llama.cpp from source and manage the build directory.
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[[commands]]
name = "ðŸ“¥ Setup Tutorial"
description = "Access the official GitHub releases page to download pre-compiled binaries for your machine. Rename the unzipped binary folder to 'llama.cpp'."
col = "A"
row = 0
items = [
  { name = "Open Releases Page", description = "Opens the latest llama.cpp releases page in your browser. Download a pre-compiled release, unpack it, and rename the main directory to 'llama.cpp', placing it in your home folder (~/llama.cpp/llama-server, llama-cli, ...etc) to match the expected paths in Quick Start.", command = "xdg-open https://github.com/ggerganov/llama.cpp/releases/latest >/dev/null 2>&1 &", auto_close_execution = true },
  { name = "Open Target Directory", description ="Once the compressed file is downloaded, move the compressed file to this destination. Unpack the file and rename the folder to 'llama.cpp'. Make sure the binary files are not in a subfolder!", command = "xdg-open $HOME", auto_close_execution = true }
]

[[commands]]
name = "ðŸ›‘ Stop All Services"
description = "Stops all running llama-server processes."
col = "A"
row = 2
items = [
  { name = "Stop All Servers", description = "Finds and terminates all running llama-server instances.", command = """
#!/bin/bash

# Check for either llama-server OR llama-cli
if pgrep -f "llama-server|llama-cli" > /dev/null; then
    # Kill both matches at once
    pkill -f "llama-server|llama-cli"
    echo "All llama-server and llama-cli processes have been stopped."
else
    echo "No llama processes (server or cli) were found running."
fi

""", auto_close_execution = false }
]

#=======================================================================================================================
# â”Œâ”€ COLUMN B: Local Usage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Run inference and benchmarks directly on your local machine.
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[[commands]]
name = "ðŸ¦™ Quick Start (TUI)"
description = "Launch a command to download models. These commands run in the foreground so you can see download progress. Once the model download is complete (TUI starts), you can safely Ctrl+C. Models are saved to ~/.cache/llama.cpp."
col = "B"
row = 0
items = [
    { name = "Download & Run CPU (Qwen3-0.6B)", description = "Downloads the small and fast Qwen3-0.6B model in the foreground.", command = "x-terminal-emulator -e $HOME/llama.cpp/llama-cli -hf ggml-org/Qwen3-0.6B-GGUF --jinja -c 0 > /dev/null 2>&1 &" },
    #{ name = "Download & Run CPU (Qwen2.5-VL-3B)", description = "Downloads the Qwen2.5-VL-3B Vision Language model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen2.5-VL-3B-Instruct-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033", auto_close_execution = false },
    { name = "Download & Run CPU (Qwen2.5-Omni-3B-GGUF)", description = "Downloads the Qwen2.5-Omni-3B-GGUF model in the foreground.", command = "x-terminal-emulator -e $HOME/llama.cpp/llama-cli -hf ggml-org/Qwen2.5-Omni-3B-GGUF --jinja -c 0 > /dev/null 2>&1 &"},
    { name = "[MacOS] Download & Run CPU (Qwen2.5-Omni-3B-GGUF)", description = "Downloads the Qwen2.5-Omni-3B-GGUF model in the foreground. [MacOS] This command serves as an example of how one could implement related commands, since x-terminal-emulator is not an option. Better MacOS support will come in time.", command = "echo '$HOME/llama.cpp/llama-cli -hf ggml-org/Qwen2.5-Omni-3B-GGUF --jinja -c 0' > /tmp/llama_launch.command && chmod +x /tmp/llama_launch.command && open /tmp/llama_launch.command > /dev/null 2>&1 &"},
    { name = "Download & Run CPU (Qwen3-4B-Thinking)", description = "Downloads the Qwen3-4B-Thinking-2507-Q8_0 model in the foreground.", command = "x-terminal-emulator -e $HOME/llama.cpp/llama-cli -hf ggml-org/Qwen3-4B-Thinking-2507-Q8_0-GGUF --jinja -c 16000 > /dev/null 2>&1 &"},
   # { name = "Download & Run CPU (Ministral 3B)", description = "Downloads the Ministral 3B Reasoning model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-3B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033", auto_close_execution = false },
   # { name = "Download & Run CPU (Ministral 8B)", description = "Downloads the Ministral 8B Reasoning model in the foreground.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-8B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033", auto_close_execution = false },
    { name = "Download & Run GPU (GPT-OSS-20B)", description = "Downloads GPT-OSS-20B (MXFP4). Optimized for NVIDIA 3060 (12GB). Runs in foreground.", command = "x-terminal-emulator -e $HOME/llama.cpp/llama-cli -hf ggml-org/gpt-oss-20b-GGUF --jinja -c 8192 -ngl 10 --n-cpu-moe 12 -b 2048 -ub 2048 --cont-batching --no-mmap > /dev/null 2>&1 &" }
]

[[commands]]
name = "ðŸŒ Launch WebUI Service"
description = "Launch the server with sensible defaults. Assumes models are already downloaded. These run in the background.WebUI access :  http://127.0.0.1:8033"
col = "B"
row = 1
items = [
    {name = "Open Local WebUI)", description = "Opens the LLamaCPP WebUI address in your Browser.", command = "xdg-open http://127.0.0.1:8033/ > /dev/null 2>&1 &"},
    { name = "Start CPU (Qwen3-0.6B)", description = "Launches the small and fast Qwen3-0.6B model.", command = "$HOME/llama.cpp/llama-server -hf ggml-org/Qwen3-0.6B-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    #{ name = "Start CPU (Qwen2.5-VL-3B)", description = "Launches the Qwen2.5-VL-3B Vision Language model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Qwen2.5-VL-3B-Instruct-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start CPU (Qwen2.5-Omni-3B-GGUF)", description = "Launches the Qwen2.5-Omni-3B-GGUF model.", command = "$HOME/llama.cpp/llama-server -hf ggml-org/Qwen2.5-Omni-3B-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start CPU (Qwen3-4B-Thinking)", description = "Launches the Qwen3-4B-Thinking-2507-Q8_0 model.", command = "$HOME/llama.cpp/llama-server -hf ggml-org/Qwen3-4B-Thinking-2507-Q8_0-GGUF --jinja -c 16000 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    #{ name = "Start CPU (Ministral 3B)", description = "Launches the Ministral 3B Reasoning model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-3B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
#    { name = "Start CPU (Ministral 8B)", description = "Launches the Ministral 8B Reasoning model.", command = "$HOME/llama.cpp/build/bin/llama-server -hf ggml-org/Ministral-3-8B-Reasoning-2512-GGUF --jinja -c 8192 --host 127.0.0.1 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
    { name = "Start GPU Server (GPT-OSS-20B)", description = "Launches GPT-OSS-20B. Optimized for NVIDIA 3060 (12GB). Network available (0.0.0.0).", command = "$HOME/llama.cpp/llama-server -hf ggml-org/gpt-oss-20b-GGUF --jinja -c 8192 -ngl 10 --n-cpu-moe 12 -b 2048 -ub 2048 --cont-batching --no-mmap --host 0.0.0.0 --port 8033 > ~/llama.cpp/server.log 2>&1 &", auto_close_execution = false },
   #{ name = "Open Remote WebUI", description = "Opens a (remote) LLamaCPP WebUI address in your Browser.", command = "xdg-open http://0.0.0.0:8033/ > /dev/null 2>&1 &"},
]

#=======================================================================================================================
# â”Œâ”€ COLUMN C: Models & Docs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Manage Models
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[[commands]]
name = "ðŸ’¾ Manage Models"
description = "Manage GGUF models downloaded to the local cache (~/.cache/llama.cpp)."
col = "C"
row = 0
items = [
    { name = "List Models", description = "List all GGUF models and their sizes.", command = """
#!/bin/bash
echo "ðŸ“‚ Location: ~/.cache/llama.cpp/"
echo "---------------------------------------------------"
du -h ~/.cache/llama.cpp/*.gguf | awk '{print $1, $2}' | sed "s|$HOME/.cache/llama.cpp/||"
echo "---------------------------------------------------"
du -sh ~/.cache/llama.cpp
""", auto_close_execution = false },
    { name = "Remove Model", description = "Select and delete a specific model file and its metadata.", command = """
#!/bin/bash
cd ~/.cache/llama.cpp || { echo "Cache dir not found"; exit 1; }
files=(*.gguf)
if [ ${#files[@]} -eq 0 ]; then
    echo "No models found."
    exit 0
fi
echo "Select a model to DELETE:"
select file in "${files[@]}"; do
    if [ -n "$file" ]; then
        read -p "âš ï¸  Are you sure you want to delete '$file'? (y/N) " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            rm -v "$file"
            # Try to remove associated metadata if it exists
            rm -vf "${file}.etag"
        else
            echo "Cancelled."
        fi
        break
    else
        echo "Invalid selection."
    fi
done
""", auto_close_execution = false }
]

[[commands]]
name = "ðŸ“š Documentation"
description = "Access official llama.cpp documentation and resources."
col = "C"
row = 1
items = [
    { name = "Open llama.cpp GitHub", description = "Explore the source code and latest updates.", command = "xdg-open https://github.com/ggerganov/llama.cpp >/dev/null 2>&1 &" },
    { name = "Learn about GGUF", description = "Read about the GGUF model format.", command = "xdg-open https://github.com/ggerganov/ggml/blob/master/docs/gguf.md >/dev/null 2>&1 &"}
]

[[commands]]
name = " â±ï¸ Benchmarks"
description = "Measure model performance with llama-bench."
col = "C"
row = 2
items = [
    { name = "Run Benchmark", description = "Prompts for a model path and parameters to run a benchmark.", command = """
#!/bin/bash
read -p 'Enter path to GGUF model file: ' model_path
read -p 'Enter number of threads (e.g., 8): ' threads
read -p 'Enter number of GPU layers (e.g., 35): ' ngl
$HOME/llama.cpp/build/bin/llama-bench -hf "$model_path" -t "$threads" -ngl "$ngl" > ~/llama.cpp/server.log 2>&1 &
""", auto_close_execution = false }
]
